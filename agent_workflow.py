# agent_workflow.py

import pandas as pd
import numpy as np
import logging
import json
import argparse
import sys
import os
from datetime import datetime
import io # Import io for StringIO
import h2o # Import H2O
from h2o.automl import H2OAutoML # Import H2OAutoML

# --- Fix ImportError START ---
# Add the project root directory (which contains agent_workflow.py and the src folder)
# to the Python path to ensure modules in src can be imported.
project_root = os.path.dirname(os.path.abspath(__file__))
if project_root not in sys.path:
    sys.path.insert(0, project_root)
# --- Fix ImportError END ---

# Ensure the src directory is in the Python path
# This might be necessary if running from the root directory
# sys.path.insert(0, os.path.abspath(os.path.join(os.path.dirname(__file__), 'src')))
# Better approach: Add src to PYTHONPATH or run from root and use `from src.ml_agent import ...`
try:
    from src.ml_agent import llm_data_analyzer, llm_data_planner, data_preparer
    from src.ml_agent import llm_data_evaluator
    # Add imports for new modules
    from src.ml_agent import feature_engineer
    from src.ml_agent import h2o_executor
    # 取消注释以导入特征工程和模型配置模块
    from src.ml_agent import llm_feature_planner
    from src.ml_agent import llm_feature_evaluator
    from src.ml_agent import llm_model_configurator
    from src.ml_agent import llm_model_evaluator
except ImportError as e: # Capture the exception object as 'e'
     # Handle case where script is run from within src/ or similar issues
     # This is less robust, structuring the project and PYTHONPATH is better
     print(f"Error: Could not import modules. Check paths and imports. Original error: {e}", file=sys.stderr)
     print(f"Current sys.path: {sys.path}", file=sys.stderr) # Print sys.path for debugging
     sys.exit(1)

# Define output directories
LOGS_DIR = "logs"
REPORTS_DIR = "reports"
DATA_DIR = "data" # Assuming input data might also be here, output will go here too
MODELS_DIR = "models" # For saving H2O models

# Configure logging
log_formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')
logger = logging.getLogger('AgentWorkflow') # Give logger a specific name
logger.setLevel(logging.INFO)
logger.propagate = False # Prevent duplicate logging if root logger is configured

# Remove existing handlers if any (useful for re-runs in notebooks/interactive)
if logger.hasHandlers():
    logger.handlers.clear()

# Create logs directory if it doesn't exist
os.makedirs(LOGS_DIR, exist_ok=True)
log_filename = os.path.join(LOGS_DIR, f'agent_workflow_{datetime.now().strftime("%Y%m%d_%H%M%S")}.log')
file_handler = logging.FileHandler(log_filename)
file_handler.setFormatter(log_formatter)
logger.addHandler(file_handler)

# Console handler for high-level messages
console_handler = logging.StreamHandler(sys.stdout) # Use stdout for console
console_handler.setLevel(logging.INFO) # Show INFO level messages on console
console_handler.setFormatter(logging.Formatter('%(levelname)s (AgentWorkflow): %(message)s'))
logger.addHandler(console_handler)


def execute_data_prep_plan(df: pd.DataFrame, plan: list, analysis_report: dict) -> (pd.DataFrame, dict):
    """
    Executes the data preparation plan step-by-step. (Renamed from execute_plan)

    Args:
        df (pd.DataFrame): The input DataFrame.
        plan (list): The list of steps (dictionaries) generated by the planner.
        analysis_report (dict): The analysis report from the analyzer (used for context, e.g., finding ID columns).

    Returns:
        tuple: A tuple containing:
            - pd.DataFrame: The processed DataFrame.
            - dict: A dictionary containing fitted objects (scalers, encoders).
    """
    logger.info(f"--- Starting Data Preparation Plan Execution ({len(plan)} steps) ---")
    df_processed = df.copy()
    fitted_objects = {} # To store scalers, encoders, etc.

    # Handle analysis_report properly
    if isinstance(analysis_report, str):
        try:
            analysis_report = json.loads(analysis_report)
        except json.JSONDecodeError:
            logger.warning(f"Analysis report is a string and couldn't be parsed as JSON")
            analysis_report = {"overall_summary": analysis_report}

    # Identify potential ID columns from analysis report to exclude from processing like scaling
    id_columns = []
    if isinstance(analysis_report, dict):
        column_analysis = analysis_report.get('column_analysis', [])
        if isinstance(column_analysis, list):
            id_columns = [
                col_info.get('column_name')
                for col_info in column_analysis
                if isinstance(col_info, dict) and col_info.get('inferred_semantic_type') == 'ID'
            ]
    
    if id_columns:
        logger.info(f"Identified potential ID columns from analysis: {id_columns}")

    # Get target variable
    target_variable = None
    if isinstance(analysis_report, dict):
        # Try direct access first
        target_variable = analysis_report.get('potential_target_variable')
        # Try nested in overall_summary if it's a dict
        if target_variable is None and isinstance(analysis_report.get('overall_summary'), dict):
            target_variable = analysis_report.get('overall_summary', {}).get('potential_target_variable')
    
    if target_variable:
        logger.info(f"Identified potential target variable from analysis: {target_variable}")

    for step in plan:
        step_num = step.get('step', 'N/A')
        module_name = step.get('module')
        func_name = step.get('function')
        # Use 'args' instead of 'params' to match feature_engineer functions
        params = step.get('args', {}) # Changed 'params' to 'args' for consistency
        reasoning = step.get('reasoning', 'No reasoning provided.')

        logger.info(f"Step {step_num}: Module='{module_name}', Function='{func_name}', Args={params}")
        logger.debug(f"Step {step_num} Reasoning: {reasoning}")

        # --- Special handling for COMMENT ---
        if func_name == "COMMENT" or module_name == "COMMENT": # Allow module name too
             # Handle special COMMENT steps, e.g., for column removal intent
             if 'message' in params:
                  logger.info(f"Plan Comment: {params['message']}")
             if 'column_to_remove' in params:
                 col_to_remove = params['column_to_remove']
                 if col_to_remove in df_processed.columns:
                     try:
                         df_processed.drop(columns=[col_to_remove], inplace=True)
                         logger.info(f"Executed COMMENT: Removed column '{col_to_remove}' as per plan intent.")
                     except Exception as e:
                         logger.error(f"Failed to remove column '{col_to_remove}' based on COMMENT: {e}")
                 else:
                     logger.warning(f"COMMENT specified removing '{col_to_remove}', but column not found (might have been removed already).")
             else:
                 logger.info(f"Ignoring COMMENT step with no actionable instruction other than message: {params}")
             continue # Move to next step


        # --- Dynamically get the function ---
        if module_name != 'data_preparer':
             logger.warning(f"Skipping step {step_num}: Expected module 'data_preparer', got '{module_name}'.")
             continue

        if not hasattr(data_preparer, func_name):
            logger.error(f"Function '{func_name}' not found in data_preparer module. Skipping step {step_num}.")
            continue

        func = getattr(data_preparer, func_name)

        try:
            # --- Pre-execution parameter adjustments ---
            cols_to_check = []
            if 'columns' in params and isinstance(params.get('columns'), list):
                 cols_to_check.extend(params['columns'])
            elif 'column' in params and isinstance(params.get('column'), str):
                 cols_to_check.append(params['column'])

            missing_in_df = [col for col in cols_to_check if col not in df_processed.columns]
            if missing_in_df:
                 logger.warning(f"Column(s) {missing_in_df} specified for function '{func_name}' not found in current DataFrame. Skipping step {step_num}.")
                 continue

            # --- Execute the function ---
            # Modify how functions are called based on their expected return signature in data_preparer
            if func_name in ['encode_label', 'scale_numerical']: # Assuming these return (df, fitted_obj)
                 df_processed, fitted_obj = func(df_processed, **params)
                 if fitted_obj:
                      param_str = params.get('column') or '_'.join(params.get('columns', []))
                      obj_suffix = "scaler" if 'scale' in func_name else "encoder"
                      obj_name = f"{func_name}_{param_str}_{obj_suffix}" if param_str else f"{func_name}_{obj_suffix}"
                      fitted_objects[obj_name] = fitted_obj
                      logger.info(f"Stored fitted object: {obj_name}")
            else:
                # Assume other functions modify inplace or return df
                result = func(df_processed, **params)
                if isinstance(result, pd.DataFrame):
                    logger.debug(f"Function '{func_name}' returned a DataFrame. Updating df_processed.")
                    df_processed = result
                else:
                    logger.debug(f"Assuming function '{func_name}' modified DataFrame inplace.")


            logger.info(f"Data Prep Step {step_num} ('{func_name}') executed successfully.")
            logger.debug(f"DataFrame shape after Data Prep step {step_num}: {df_processed.shape}")

        except Exception as e:
            logger.error(f"Error executing Data Prep step {step_num} ('{func_name}' with args {params}): {e}", exc_info=True)
            logger.warning(f"Continuing plan execution despite error in step {step_num}.")

    logger.info("--- Data Preparation Plan Execution Finished ---")
    return df_processed, fitted_objects

# NEW FUNCTION for Feature Engineering Plan
def execute_feature_plan(df: pd.DataFrame, plan: list, analysis_report: dict) -> (pd.DataFrame, dict):
    """
    Executes the feature engineering plan step-by-step.

    Args:
        df (pd.DataFrame): The input DataFrame.
        plan (list): The list of steps (dictionaries) generated by the planner.
        analysis_report (dict): The analysis report from the analyzer (used for context, e.g., finding ID columns).

    Returns:
        tuple: A tuple containing:
            - pd.DataFrame: The processed DataFrame.
            - dict: A dictionary containing fitted objects (scalers, encoders).
    """
    logger.info(f"--- Starting Feature Engineering Plan Execution ({len(plan)} steps) ---")
    df_processed = df.copy()
    fitted_fe_objects = {} # To store fitted feature engineering objects

    # Handle analysis_report properly
    if isinstance(analysis_report, str):
        try:
            analysis_report = json.loads(analysis_report)
        except json.JSONDecodeError:
            logger.warning(f"Analysis report is a string and couldn't be parsed as JSON")
            analysis_report = {"overall_summary": analysis_report}

    # Now safely access the dictionary
    id_columns = []
    if isinstance(analysis_report, dict):
        column_analysis = analysis_report.get('column_analysis', [])
        if isinstance(column_analysis, list):
            id_columns = [
                col_info.get('column_name')
                for col_info in column_analysis
                if isinstance(col_info, dict) and col_info.get('inferred_semantic_type') == 'ID'
            ]
    
    if id_columns:
        logger.info(f"Identified potential ID columns from analysis: {id_columns}")
    
    # Get target variable
    target_variable = None
    if isinstance(analysis_report, dict):
        # Try direct access first
        target_variable = analysis_report.get('potential_target_variable')
        # Try nested in overall_summary if it's a dict
        if target_variable is None and isinstance(analysis_report.get('overall_summary'), dict):
            target_variable = analysis_report.get('overall_summary', {}).get('potential_target_variable')
    
    if target_variable:
        logger.info(f"Using target variable from analysis: {target_variable}")

    for step in plan:
        step_num = step.get('step', 'N/A')
        module_name = step.get('module')
        func_name = step.get('function')
        args = step.get('args', {}) # Parameters for the feature engineering function
        reasoning = step.get('reasoning', 'No reasoning provided.')

        # Display Chinese summary if available (usually only in first step)
        if 'summary_chinese' in step:
            logger.info(f"特征工程计划中文总结: {step.get('summary_chinese')}")

        logger.info(f"Step {step_num}: Module='{module_name}', Function='{func_name}', Args={args}")
        logger.debug(f"Step {step_num} Reasoning: {reasoning}")

        # --- Special handling for COMMENT ---
        if func_name == "COMMENT" or module_name == "COMMENT":
             logger.info(f"Skipping COMMENT step: {args.get('message', 'No message provided')}")
             continue

        # --- Dynamically get the function ---
        if module_name != 'feature_engineer':
             logger.warning(f"Skipping step {step_num}: Expected module 'feature_engineer', got '{module_name}'.")
             continue

        if not hasattr(feature_engineer, func_name):
            logger.error(f"Function '{func_name}' not found in feature_engineer module. Skipping step {step_num}.")
            continue

        func = getattr(feature_engineer, func_name)

        try:
            # --- Pre-execution parameter adjustments ---
            cols_to_check = []
            if 'columns' in args and isinstance(args.get('columns'), list):
                 cols_to_check.extend(args['columns'])
            elif 'column' in args and isinstance(args.get('column'), str):
                 cols_to_check.append(args['column'])
            
            if 'columns1' in args and isinstance(args.get('columns1'), list):
                 cols_to_check.extend(args['columns1'])
            if 'columns2' in args and isinstance(args.get('columns2'), list):
                 cols_to_check.extend(args['columns2'])

            missing_in_df = [col for col in cols_to_check if col not in df_processed.columns]
            if missing_in_df:
                 logger.warning(f"Column(s) {missing_in_df} specified for function '{func_name}' not found in current DataFrame. Skipping step {step_num}.")
                 continue

            # --- Execute the function ---
            # Different handling based on function signatures
            if func_name in ['scale_numerical', 'encode_categorical', 'handle_high_cardinality']:
                 # Functions that return (df, fitted_obj)
                 df_processed, fitted_obj = func(df_processed, **args)
                 if fitted_obj is not None:
                      # Generate a unique name for the fitted object
                      param_str = args.get('column', '') or '_'.join(args.get('columns', []))
                      obj_name = f"{func_name}_{param_str}_obj"
                      fitted_fe_objects[obj_name] = fitted_obj
                      logger.info(f"Stored fitted object: {obj_name}")
            else:
                # Functions that modify inplace or return df
                result = func(df_processed, **args)
                if isinstance(result, pd.DataFrame):
                    df_processed = result
                    logger.debug(f"Function '{func_name}' returned a DataFrame. Updating df_processed.")
                else:
                    logger.debug(f"Assuming function '{func_name}' modified DataFrame inplace.")

            logger.info(f"Feature Engineering Step {step_num} ('{func_name}') executed successfully.")
            logger.debug(f"DataFrame shape after FE step {step_num}: {df_processed.shape}")

        except Exception as e:
            logger.error(f"Error executing FE step {step_num} ('{func_name}' with args {args}): {e}", exc_info=True)
            logger.warning(f"Continuing plan execution despite error in step {step_num}.")

    logger.info("--- Feature Engineering Plan Execution Finished ---")
    return df_processed, fitted_fe_objects


def main(file_path: str, user_goal: str = None, output_arg: str = None, stage: str = 'full', language: str = 'en'):
    """
    Runs the full Analyze -> Plan -> Execute (Data Prep) -> Evaluate ->
                 FE Plan -> FE Execute -> FE Evaluate ->
                 Model Config -> Model Execute (H2O) -> Final Evaluate workflow.

    Args:
        file_path (str): Path to the input CSV file.
        user_goal (str, optional): User goal for the analysis. Defaults to None.
        output_arg (str, optional): Path/directory for the output CSV. Defaults to None.
        stage (str, optional): The stage to run up to ('data_prep', 'feature_eng', 'model', 'full'). Defaults to 'full'.
        language (str, optional): Language for summaries ('en' for English, 'cn' for Chinese). Defaults to 'en'.
    """
    run_timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    logger.info(f"Starting Agent Workflow (Run ID: {run_timestamp}) for file: {file_path}")
    logger.info(f"User Goal: {user_goal if user_goal else 'Not specified'}")
    logger.info(f"Requested Stage: {stage}")
    logger.info(f"Language: {language}")

    base_name = os.path.splitext(os.path.basename(file_path))[0]

    # --- Load Data ---
    try:
        try: df = pd.read_csv(file_path)
        except UnicodeDecodeError:
            logger.warning("UTF-8 decoding failed, trying latin-1 encoding.")
            df = pd.read_csv(file_path, encoding='latin-1')
        logger.info(f"Successfully loaded data. Shape: {df.shape}")
    except Exception as e:
        logger.error(f"Error loading data from {file_path}: {e}", exc_info=True)
        return

    # ========================================
    # ===== STAGE 1: Data Preparation ======
    # ========================================
    logger.info("====== Starting Stage 1: Data Preparation ======")

    # --- Analyze Stage ---
    logger.info("--- Analyzing Data ---")
    analysis_report = llm_data_analyzer.analyze_data_with_llm(df, user_goal=user_goal, language=language)
    
    # Check if analysis_report is a string and try to parse it as JSON, otherwise ensure it's a dictionary
    if isinstance(analysis_report, str):
        try:
            # Use the already imported json module
            analysis_report = json.loads(analysis_report)
        except json.JSONDecodeError:
            logger.error(f"Failed to parse analysis_report as JSON. Creating a basic report dictionary.")
            analysis_report = {
                "overall_summary": analysis_report,
                "column_analysis": [],
                "potential_target_variable": None
            }
    
    if not isinstance(analysis_report, dict) or "error" in analysis_report:
        error_msg = analysis_report.get('error', 'Unknown error') if isinstance(analysis_report, dict) else "Invalid analysis report format"
        logger.error(f"Analyze stage failed: {error_msg}")
        if isinstance(analysis_report, dict) and "raw_response" in analysis_report: 
            logger.error(f"Raw LLM response (Analyze): {analysis_report.get('raw_response', 'N/A')}")
        return
    
    logger.info("Analyze stage completed.")
    # Select language-appropriate summary
    analysis_summary = analysis_report.get('overall_summary', 'N/A')
    if language == 'cn' and 'overall_summary_chinese' in analysis_report:
        analysis_summary = analysis_report.get('overall_summary_chinese')
    logger.info(f"Data Analysis Summary: {analysis_summary}")
    analysis_file = os.path.join(REPORTS_DIR, f'analysis_report_{base_name}_{run_timestamp}.json')
    try:
        with open(analysis_file, 'w') as f: 
            json.dump(analysis_report, f, indent=4)
        logger.info(f"Analysis report saved to {analysis_file}")
    except Exception as e: 
        logger.error(f"Failed to save analysis report: {e}")

    # --- Define id_columns and target_variable early based on analysis ---
    # If column_analysis is directly in the dictionary, use it
    column_analysis = analysis_report.get('column_analysis', [])
    if not isinstance(column_analysis, list):
        column_analysis = []
        
    id_columns = []
    for col_info in column_analysis:
        if isinstance(col_info, dict) and col_info.get('inferred_semantic_type') == 'ID':
            id_columns.append(col_info.get('column_name'))
    
    # Check if potential_target_variable is a direct key or nested in overall_summary
    target_variable = analysis_report.get('potential_target_variable')
    if target_variable is None and isinstance(analysis_report.get('overall_summary'), dict):
        target_variable = analysis_report.get('overall_summary', {}).get('potential_target_variable')
    
    if id_columns: 
        logger.info(f"Workflow identified potential ID columns: {id_columns}")
    if target_variable: 
        logger.info(f"Workflow identified potential target variable: {target_variable}")

    # --- Plan Stage (Data Prep) ---
    logger.info("--- Planning Data Preparation ---")
    try:
        # Check if generate_plan_with_llm accepts language parameter
        import inspect
        from src.ml_agent import llm_data_planner
        plan_function = llm_data_planner.generate_plan_with_llm
        plan_params = inspect.signature(plan_function).parameters
        
        if 'language' in plan_params:
            plan_steps = llm_data_planner.generate_plan_with_llm(analysis_report, user_goal=user_goal, language=language)
        else:
            # Fallback if language parameter is not accepted
            plan_steps = llm_data_planner.generate_plan_with_llm(analysis_report, user_goal=user_goal)
    except Exception as e:
        logger.error(f"Data Prep Plan generation failed: {e}")
        return
        
    # Error handling for planner
    if not isinstance(plan_steps, list) or not plan_steps:
        logger.error(f"Data Prep Plan stage failed. Received: {plan_steps}")
        # Log raw response if available in the returned object
        if isinstance(plan_steps, dict) and "raw_response" in plan_steps:
             logger.error(f"Raw LLM response (Data Prep Plan): {plan_steps.get('raw_response', 'N/A')}")
        return

    logger.info(f"Data Prep Plan generated ({len(plan_steps)} steps).")
    # Output appropriate language plan summary
    if plan_steps and isinstance(plan_steps[0], dict):
        if language == 'cn' and 'summary_chinese' in plan_steps[0]:
            logger.info(f"Data Prep Plan Summary: {plan_steps[0].get('summary_chinese')}")
        elif 'summary' in plan_steps[0]:
            logger.info(f"Data Prep Plan Summary: {plan_steps[0].get('summary')}")
    plan_file = os.path.join(REPORTS_DIR, f'dataprep_plan_{base_name}_{run_timestamp}.json')
    try:
        with open(plan_file, 'w') as f: json.dump(plan_steps, f, indent=4)
        logger.info(f"Data Prep plan saved to: {plan_file}")
    except Exception as e: logger.error(f"Error saving Data Prep plan: {e}")

    # --- Execute Stage (Data Prep) ---
    logger.info("--- Executing Data Preparation Plan ---")
    df_after_prep, fitted_prep_objects = execute_data_prep_plan(df, plan_steps, analysis_report)
    if df_after_prep is None:
        logger.error("Data Prep Execute stage failed. Halting workflow.")
        return
    logger.info("Data Prep Execute stage completed.")
    logger.info(f"DataFrame shape after Data Prep: {df_after_prep.shape}")
    logger.info(f"Fitted Data Prep objects: {list(fitted_prep_objects.keys())}")

    # --- Evaluate Stage (Data Prep) ---
    logger.info("--- Evaluating Data Preparation ---")
    try:
        # Check if evaluate_data accepts language parameter
        import inspect
        from src.ml_agent import llm_data_evaluator
        eval_function = llm_data_evaluator.evaluate_data
        eval_params = inspect.signature(eval_function).parameters
        
        if 'language' in eval_params:
            prep_eval_report = llm_data_evaluator.evaluate_data(
                processed_df=df_after_prep, 
                execution_plan=plan_steps, 
                analysis_report=analysis_report, 
                language=language
            )
        else:
            # Fallback if language parameter is not accepted
            prep_eval_report = llm_data_evaluator.evaluate_data(
                processed_df=df_after_prep, 
                execution_plan=plan_steps, 
                analysis_report=analysis_report
            )
    except Exception as e:
        logger.error(f"Data Prep Evaluate stage failed with error: {e}", exc_info=True)
        prep_eval_report = {"error": str(e), "overall_summary": "Data preparation evaluation failed."}
        
    # Error handling for evaluator
    if not isinstance(prep_eval_report, dict) or "error" in prep_eval_report:
         logger.error(f"Data Prep Evaluate stage failed: {prep_eval_report.get('error', 'Unknown error')}")
         if "raw_response" in prep_eval_report: logger.error(f"Raw LLM response (Data Prep Eval): {prep_eval_report.get('raw_response', 'N/A')}")
         # Decide if workflow should halt on eval failure
         # return # Optional: halt workflow here

    logger.info("Data Prep Evaluate stage completed.")
    # Select appropriate language summary
    eval_summary = prep_eval_report.get('overall_summary', 'N/A')
    if language == 'cn' and 'overall_summary_chinese' in prep_eval_report:
        eval_summary = prep_eval_report.get('overall_summary_chinese')
    logger.info(f"Data Prep Eval Summary: {eval_summary}")
    eval_file = os.path.join(REPORTS_DIR, f'dataprep_eval_report_{base_name}_{run_timestamp}.json')
    try:
        with open(eval_file, 'w') as f: json.dump(prep_eval_report, f, indent=4)
        logger.info(f"Data Prep Evaluation report saved to: {eval_file}")
    except Exception as e: logger.error(f"Error saving Data Prep evaluation report: {e}")

    # --- Stop if only data_prep stage was requested ---
    if stage == 'data_prep':
        logger.info(f"Workflow finished after 'data_prep' stage as requested.")
        # Save the intermediate data
        intermediate_data_path = os.path.join(DATA_DIR, f'{base_name}_processed_dataprep_{run_timestamp}.csv')
        try:
            df_after_prep.to_csv(intermediate_data_path, index=False)
            logger.info(f"Data after Data Prep saved to {intermediate_data_path}")
        except Exception as e:
            logger.error(f"Error saving data after Data Prep stage: {e}", exc_info=True)
        return # Stop execution

    # ==============================================
    # ===== STAGE 2: Feature Engineering =========
    # ==============================================
    logger.info("====== Starting Stage 2: Feature Engineering ======")
    df_current = df_after_prep # Pass data to next stage

    # --- Plan Stage (Feature Eng) ---
    logger.info("--- Planning Feature Engineering ---")
    try:
        fe_plan_steps = llm_feature_planner.generate_fe_plan(
            analysis_report=analysis_report,
            data_so_far=df_current,
            prep_evaluation=prep_eval_report,
            user_instructions=user_goal,
            language=language
        )
        # Display appropriate language summary
        if fe_plan_steps and isinstance(fe_plan_steps[0], dict):
            if language == 'cn' and 'summary_chinese' in fe_plan_steps[0]:
                logger.info(f"Feature Engineering Plan Summary: {fe_plan_steps[0].get('summary_chinese')}")
            elif 'summary' in fe_plan_steps[0]:
                logger.info(f"Feature Engineering Plan Summary: {fe_plan_steps[0].get('summary')}")
    except Exception as e:
        logger.error(f"Feature Eng Plan stage failed with error: {e}", exc_info=True)
        return

    # Error handling for FE planner
    if not isinstance(fe_plan_steps, list) or not fe_plan_steps:
         logger.error(f"Feature Eng Plan stage failed. LLM did not return a valid plan list. Received: {fe_plan_steps}")
         if isinstance(fe_plan_steps, dict) and "raw_response" in fe_plan_steps:
             logger.error(f"Raw LLM response (Feature Eng Plan): {fe_plan_steps.get('raw_response', 'N/A')}")
         return

    logger.info(f"Feature Eng Plan generated ({len(fe_plan_steps)} steps).")
    fe_plan_file = os.path.join(REPORTS_DIR, f'featureeng_plan_{base_name}_{run_timestamp}.json')
    try:
        with open(fe_plan_file, 'w') as f: json.dump(fe_plan_steps, f, indent=4)
        logger.info(f"Feature Eng plan saved to: {fe_plan_file}")
    except Exception as e: logger.error(f"Error saving Feature Eng plan: {e}")

    # --- Execute Stage (Feature Eng) ---
    logger.info("--- Executing Feature Engineering Plan ---")
    df_after_fe, fitted_fe_objects = execute_feature_plan(df_current, fe_plan_steps, analysis_report)
    if df_after_fe is None:
        logger.error("Feature Eng Execute stage failed. Halting workflow.")
        return
    logger.info("Feature Eng Execute stage completed.")
    logger.info(f"DataFrame shape after Feature Eng: {df_after_fe.shape}")
    logger.info(f"Fitted Feature Eng objects: {list(fitted_fe_objects.keys())}")

    # --- Evaluate Stage (Feature Eng) ---
    logger.info("--- Evaluating Feature Engineering ---")
    try:
        fe_eval_report = llm_feature_evaluator.evaluate_fe_data(
            df_processed=df_after_fe,
            fe_plan=fe_plan_steps,
            analysis_report=analysis_report,
            language=language
        )
        # Display appropriate language summary
        if language == 'cn' and 'overall_summary_chinese' in fe_eval_report:
            logger.info(f"Feature Engineering Evaluation Summary: {fe_eval_report.get('overall_summary_chinese')}")
        elif 'overall_summary' in fe_eval_report:
            logger.info(f"Feature Engineering Evaluation Summary: {fe_eval_report.get('overall_summary')}")
    except Exception as e:
        logger.error(f"Feature Eng Evaluate stage failed with error: {e}", exc_info=True)
        return

    # Error handling for FE evaluator
    if not isinstance(fe_eval_report, dict) or "error" in fe_eval_report:
         logger.error(f"Feature Eng Evaluate stage failed: {fe_eval_report.get('error', 'Unknown error')}")
         if "raw_response" in fe_eval_report: logger.error(f"Raw LLM response (Feature Eng Eval): {fe_eval_report.get('raw_response', 'N/A')}")
         return

    logger.info("Feature Eng Evaluate stage completed.")
    fe_eval_file = os.path.join(REPORTS_DIR, f'featureeng_eval_report_{base_name}_{run_timestamp}.json')
    try:
        with open(fe_eval_file, 'w') as f: json.dump(fe_eval_report, f, indent=4)
        logger.info(f"Feature Eng Evaluation report saved to: {fe_eval_file}")
    except Exception as e: logger.error(f"Error saving Feature Eng evaluation report: {e}")

    # --- Stop if only feature_eng stage was requested ---
    if stage == 'feature_eng':
        logger.info(f"Workflow finished after 'feature_eng' stage as requested.")
        # Save the intermediate data
        intermediate_data_path = os.path.join(DATA_DIR, f'{base_name}_processed_featureeng_{run_timestamp}.csv')
        try:
            df_after_fe.to_csv(intermediate_data_path, index=False)
            logger.info(f"Data after Feature Eng saved to {intermediate_data_path}")
        except Exception as e:
            logger.error(f"Error saving data after Feature Eng stage: {e}", exc_info=True)
        return # Stop execution

    # ==============================================
    # ===== STAGE 3: Model Agent (H2O AutoML) ====
    # ==============================================
    logger.info("====== Starting Stage 3: Model Agent (H2O AutoML) ======")
    df_final = df_after_fe # Pass final data to H2O stage

    # --- Configure Stage (Model Agent) ---
    logger.info("--- Configuring H2O AutoML ---")
    try:
        h2o_config = llm_model_configurator.generate_h2o_config(
            analysis_report=analysis_report,
            fe_evaluation=fe_eval_report,
            data_snapshot=df_final.head(20),
            user_instructions=user_goal,
            language=language,
            execution_plan=fe_plan_steps
        )
        # Display appropriate language summary
        if language == 'cn' and 'summary_chinese' in h2o_config:
            logger.info(f"H2O AutoML Configuration Summary: {h2o_config.get('summary_chinese')}")
        elif 'summary' in h2o_config:
            logger.info(f"H2O AutoML Configuration Summary: {h2o_config.get('summary')}")
    except Exception as e:
        logger.error(f"Model Config stage failed with error: {e}", exc_info=True)
        return

    # Error handling for Model Configurator
    if not isinstance(h2o_config, dict) or "error" in h2o_config:
         logger.error(f"Model Config stage failed: {h2o_config.get('error', 'Unknown error')}")
         if "raw_response" in h2o_config: logger.error(f"Raw LLM response (Model Config): {h2o_config.get('raw_response', 'N/A')}")
         return

    # Ensure necessary keys are present before proceeding
    if 'target_variable' not in h2o_config or 'h2o_automl_parameters' not in h2o_config:
        logger.error(f"Model Config stage failed: LLM response missing 'target_variable' or 'h2o_automl_parameters'. Response: {h2o_config}")
        return

    logger.info(f"H2O Params: {h2o_config.get('h2o_automl_parameters', {})}")
    h2o_config_file = os.path.join(REPORTS_DIR, f'h2o_config_{base_name}_{run_timestamp}.json')
    try:
        with open(h2o_config_file, 'w') as f: json.dump(h2o_config, f, indent=4)
        logger.info(f"H2O config saved to: {h2o_config_file}")
    except Exception as e: logger.error(f"Error saving H2O config: {e}")

    # --- Execute Stage (Model Agent - H2O) ---
    logger.info("--- Executing H2O AutoML ---")
    h2o_results = {}
    try:
        h2o_results = h2o_executor.run_h2o_automl(
            data_raw=df_final,
            target_variable=h2o_config['target_variable'],
            h2o_params=h2o_config['h2o_automl_parameters'],
            model_directory=os.path.join(MODELS_DIR, f"{base_name}_{run_timestamp}")
        )
        logger.info("H2O AutoML execution completed.")
        if h2o_results.get('leaderboard') is not None:
             leaderboard_df = pd.DataFrame(h2o_results['leaderboard'])
             logger.info("H2O Leaderboard:\n" + leaderboard_df.to_string())
        if h2o_results.get('best_model_path'):
             logger.info(f"Best H2O model saved to: {h2o_results['best_model_path']}")

    except Exception as e:
        logger.error(f"H2O AutoML execution failed: {e}", exc_info=True)
        h2o_results['error'] = str(e)

    # --- Evaluate Stage (Model Agent - Final) ---
    logger.info("--- Evaluating Final Model Results ---")
    try:
        final_eval_report = llm_model_evaluator.evaluate_final_results(
            h2o_results=h2o_results,
            user_goal=user_goal,
            initial_analysis=analysis_report,
            fe_evaluation=fe_eval_report,
            language=language
        )
        # Display appropriate language summary
        if language == 'cn' and 'overall_summary_chinese' in final_eval_report:
            logger.info(f"Model Evaluation Summary: {final_eval_report.get('overall_summary_chinese')}")
        elif 'overall_summary' in final_eval_report:
            logger.info(f"Model Evaluation Summary: {final_eval_report.get('overall_summary')}")
    except Exception as e:
        logger.error(f"Final Evaluate stage failed with error: {e}", exc_info=True)
        return

    # Error handling for Final Evaluator
    if not isinstance(final_eval_report, dict) or "error" in final_eval_report:
         logger.error(f"Final Evaluate stage failed: {final_eval_report.get('error', 'Unknown error')}")
         if "raw_response" in final_eval_report: logger.error(f"Raw LLM response (Final Eval): {final_eval_report.get('raw_response', 'N/A')}")
         return

    logger.info("Final Evaluate stage completed.")
    final_eval_file = os.path.join(REPORTS_DIR, f'final_eval_report_{base_name}_{run_timestamp}.json')
    try:
        with open(final_eval_file, 'w') as f: json.dump(final_eval_report, f, indent=4)
        logger.info(f"Final Evaluation report saved to: {final_eval_file}")
    except Exception as e: logger.error(f"Error saving Final evaluation report: {e}")


    # --- Save Final Processed Data ---
    final_data_path = os.path.join(DATA_DIR, f'{base_name}_processed_final_{run_timestamp}.csv')
    try:
        df_final.to_csv(final_data_path, index=False)
        logger.info(f"Final data state (input to H2O) saved to {final_data_path}")
    except Exception as e:
        logger.error(f"Error saving final processed data to {final_data_path}: {e}", exc_info=True)

    logger.info(f"====== Full Agent Workflow Finished (Run ID: {run_timestamp}) ======")
    logger.info(f"Detailed logs in: {log_filename}")


if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="ML Agent Multi-Stage Workflow (DataPrep->FE->Model)")
    parser.add_argument("file_path", help="Path to the input CSV file.")
    parser.add_argument("-g", "--goal", help="Optional user goal for the analysis (e.g., 'Predict target_column').", default=None)
    parser.add_argument("-o", "--output", help="Optional path or directory for the processed CSV file. If not specified, saves to 'data/[input_filename]_processed.csv'.", default=None)
    # Add stage argument
    parser.add_argument("--stage", help="Run workflow up to a specific stage.", choices=['data_prep', 'feature_eng', 'model', 'full'], default='full')
    # Add language argument
    parser.add_argument("--language", help="Language for summaries and comments.", choices=['en', 'cn'], default='en')

    args = parser.parse_args()

    if not os.path.isfile(args.file_path):
        print(f"Error: Input file not found at '{args.file_path}'", file=sys.stderr)
        sys.exit(1)

    main(args.file_path, args.goal, args.output, args.stage, args.language)