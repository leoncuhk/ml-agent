import pandas as pd
import json
import logging
from typing import Dict, Any, Optional

# Assume an LLM client/API call function exists, e.g., call_llm_api
# from .llm_utils import call_llm_api # Placeholder import

logger = logging.getLogger(__name__)

def add_chinese_summary(evaluation_result: dict) -> dict:
    """
    为评估结果添加中文总结
    
    Args:
        evaluation_result: 原始评估结果字典
        
    Returns:
        添加了中文总结的评估结果字典
    """
    if not isinstance(evaluation_result, dict):
        return evaluation_result
    
    # 如果已有中文总结，则直接返回
    if "overall_summary_chinese" in evaluation_result:
        return evaluation_result
    
    # 提取关键信息
    quality_score = evaluation_result.get('quality_score', 'N/A')
    readiness_score = evaluation_result.get('readiness_score', 'N/A')
    overall_assessment = evaluation_result.get('overall_assessment', '')
    next_steps = evaluation_result.get('suggested_next_steps', [])
    resolved_issues = evaluation_result.get('resolved_issues', [])
    remaining_issues = evaluation_result.get('remaining_issues', [])
    
    # 构建中文总结
    summary = "数据准备评估完成。"
    
    # 添加质量和就绪分数
    if quality_score != 'N/A':
        summary += f"数据质量得分：{quality_score}/10。"
    if readiness_score != 'N/A':
        summary += f"模型就绪度得分：{readiness_score}/10。"
    
    # 添加已解决的问题
    if resolved_issues:
        if len(resolved_issues) <= 3:
            summary += f"已解决的问题：{'、'.join(resolved_issues)}。"
        else:
            summary += f"已解决了{len(resolved_issues)}个数据问题。"
    
    # 添加未解决的问题
    if remaining_issues:
        if len(remaining_issues) <= 3:
            summary += f"仍存在的问题：{'、'.join(remaining_issues[:3])}。"
        else:
            summary += f"仍有{len(remaining_issues)}个问题需要处理。"
    
    # 添加下一步建议
    if next_steps and len(next_steps) > 0:
        summary += f"建议下一步：{next_steps[0]}。"
    
    # 将中文总结添加到结果中
    evaluation_result["overall_summary_chinese"] = summary
    
    return evaluation_result

def evaluate_data(
    processed_df: pd.DataFrame,
    analysis_report: Dict[str, Any],
    execution_plan: Dict[str, Any],
    user_goal: Optional[str] = None,
    llm_client: Optional[Any] = None # Pass LLM client if needed
) -> Dict[str, Any]:
    """
    Evaluates the processed data using an LLM.

    Args:
        processed_df: The DataFrame after applying the execution plan.
        analysis_report: The original analysis report generated by the analyzer.
        execution_plan: The execution plan generated by the planner.
        user_goal: The optional user-defined goal for the data processing.
        llm_client: An optional LLM client instance.

    Returns:
        A dictionary containing the LLM's evaluation report.
    """
    logger.info("Starting Data Evaluation Stage...")

    if processed_df.empty:
        logger.warning("Processed DataFrame is empty. Skipping evaluation.")
        return {"evaluation_summary": "Skipped evaluation due to empty processed DataFrame."}

    # --- 1. Prepare Data Summary for LLM ---
    processed_summary = {
        "num_rows": len(processed_df),
        "num_columns": len(processed_df.columns),
        "column_names": processed_df.columns.tolist(),
        "data_types": processed_df.dtypes.astype(str).to_dict(),
        "missing_values_per_column": processed_df.isnull().sum().to_dict(),
        "sample_head": processed_df.head().to_dict(orient='records'),
        "sample_tail": processed_df.tail().to_dict(orient='records'),
        "description": processed_df.describe(include='all').to_string() # Basic stats
    }

    # --- 2. Construct Prompt ---
    prompt_parts = [
        "**Data Evaluation Task:**",
        "You are an AI assistant evaluating the results of an automated data processing workflow.",
        "The workflow followed these stages: Analyze -> Plan -> Execute.",
        "Your task is to evaluate the quality and readiness of the *processed* data based on the initial analysis, the planned steps, and the execution results.",
        "\n**User Goal (if provided):**",
        f"{user_goal if user_goal else 'Not specified.'}",
        "\n**1. Initial Analysis Report Summary:**",
        f"{json.dumps(analysis_report, indent=2)}", # Include relevant parts or summary
        "\n**2. Execution Plan:**",
        f"{json.dumps(execution_plan, indent=2)}",
        "\n**3. Processed Data Summary:**",
        f"{json.dumps(processed_summary, indent=2)}",
        "\n**Evaluation Instructions:**",
        "Based on all the information above, please evaluate the following:",
        "   a. **Data Quality:** Assess the overall quality of the *processed* data (completeness, consistency, accuracy based on applied steps). Were the issues identified in the analysis report addressed effectively by the execution plan?",
        "   b. **Readiness for ML:** Is the processed data suitable for downstream machine learning tasks (e.g., feature engineering, modeling)? Are there any obvious remaining issues (e.g., unhandled categorical features needing encoding, unresolved outliers, data leakage potential)?",
        "   c. **Alignment with Goal:** Does the processed data seem aligned with the user's stated goal (if provided)?",
        "   d. **Plan Execution:** Were the steps in the execution plan likely followed correctly based on the comparison between the analysis report and the processed data summary?",
        "   e. **Suggestions:** Provide specific suggestions for improvement or next steps if any issues are identified.",
        "\n**Output Format:** Please provide your evaluation as a structured JSON object with keys: 'data_quality_assessment', 'ml_readiness_assessment', 'goal_alignment_assessment', 'plan_execution_assessment', 'suggestions', 'overall_summary'."
    ]
    prompt = "\n".join(prompt_parts)

    # --- 3. Call LLM API ---
    logger.info("Sending data evaluation request to LLM...")
    try:
        # response_text = call_llm_api(prompt, client=llm_client) # Replace with actual API call
        # Placeholder response for now
        logger.warning("LLM call is currently mocked. Returning placeholder evaluation.")
        response_text = json.dumps({
            "data_quality_assessment": "Mocked: Quality appears improved based on plan.",
            "ml_readiness_assessment": "Mocked: Data seems ready for basic ML tasks, but feature engineering might be needed.",
            "goal_alignment_assessment": "Mocked: Alignment with goal seems reasonable.",
            "plan_execution_assessment": "Mocked: Plan steps appear to be reflected in the processed data summary.",
            "suggestions": ["Mocked: Consider generating interaction features.", "Mocked: Verify PII removal if applicable."],
            "overall_summary": "Mocked: The data processing workflow seems to have executed successfully, producing data suitable for the next stage, though further feature engineering is recommended."
        })
        evaluation_result = json.loads(response_text)
        logger.info("Received evaluation report from LLM.")
    except Exception as e:
        logger.error(f"Error during LLM call or parsing for evaluation: {e}", exc_info=True)
        evaluation_result = {"error": str(e), "evaluation_summary": "Failed to get evaluation from LLM."}

    # --- 4. Return Evaluation ---
    # 在返回结果前添加中文总结
    evaluation_result = add_chinese_summary(evaluation_result)
    
    # 记录中文总结
    if "overall_summary_chinese" in evaluation_result:
        logger.info(f"数据准备评估中文总结: {evaluation_result['overall_summary_chinese']}")
    
    return evaluation_result 